import os
import pickle
import pandas as pd
import wandb
from tabulate import tabulate
from datetime import datetime, timezone

# --- ÏÑ§Ï†ï --------------------------------------------------
PROJECT    = "l2p_bp/mqfp_new"
CACHE_FILE = "runs_records.pkl"

VALID_PAIRS = [
    ("ppgbp","bcg"), ("ppgbp","sensors"), ("bcg","ppgbp"), ("sensors","ppgbp"),
    ("bcg","sensors"), ("sensors","bcg"),
    ("mimic_ecg","vital_ecg"), ("vital_ecg","mimic_ecg")
]
# -----------------------------------------------------------

api = wandb.Api()

def load_cache():
    if not os.path.exists(CACHE_FILE):
        return [], None
    data = pickle.load(open(CACHE_FILE, "rb"))
    last = datetime.fromisoformat(data["last_fetch"])
    return data["records"], last

def save_cache(records, last_fetch):
    data = {
        "records":    records,
        "last_fetch": last_fetch.astimezone(timezone.utc).isoformat()
    }
    pickle.dump(data, open(CACHE_FILE, "wb"), protocol=pickle.HIGHEST_PROTOCOL)

def get_latest_run_time():
    """W&BÏóêÏÑú valid_pairs Ï†ÑÏ≤¥ Ï§ë ÏµúÏã† Ìïú Í±¥Îßå Í∞ÄÏ†∏ÏôÄÏÑú createdAt Î∞òÌôò"""
    or_filters = [{"$and":[{"config.transfer":t},{"config.target":u}]} 
                  for t,u in VALID_PAIRS]
    latest = next(api.runs(
        PROJECT,
        filters={"$or": or_filters},
        order="-createdAt",
        per_page=1
    ), None)

    if latest is None:
        return None
    # run.created_atÏùÄ ISO Î¨∏ÏûêÏó¥
    iso = latest.created_at.replace("Z", "+00:00")
    return datetime.fromisoformat(iso)

def fetch_new_records(existing, last_fetch):
    """last_fetch Ïù¥ÌõÑÏóê ÏÉùÏÑ±Îêú Îü∞Îßå Í∞ÄÏ†∏ÏôÄÏÑú recordsÏóê append"""
    or_filters = [{"$and":[{"config.transfer":t},{"config.target":u}]} 
                  for t,u in VALID_PAIRS]
    # AND Ï°∞Ìï©ÏúºÎ°ú createdAt ÌïÑÌÑ∞ÍπåÏßÄ
    filters = {"$and": [
        {"$or": or_filters},
        {"createdAt": {"$gt": last_fetch.astimezone(timezone.utc).isoformat()}}
    ]}

    runs = api.runs(PROJECT, filters=filters, per_page=500)
    new_recs = []
    max_fetch = last_fetch

    for run in runs:
        created = datetime.fromisoformat(run.created_at.replace("Z","+00:00"))
        tran, tgt = run.config.get("transfer"), run.config.get("target")
        if (tran,tgt) not in VALID_PAIRS:
            continue

        for metric in ("gal","spdp"):
            val = run.summary.get(metric)
            if val is None:
                continue
            new_recs.append({
                "run_id":     run.id,
                "created_at": created,
                "backbone":   run.config.get("backbone",""),
                "method":     run.config.get("method",""),
                "transfer":   tran,
                "target":     tgt,
                "metric":     metric,
                "value":      val,
                "shots":      run.config.get("shots","N/A"),
                "train_head": run.config.get("train_head","N/A"),
                "baseline":   run.config.get("baseline","N/A")
            })
        if created > max_fetch:
            max_fetch = created

    return existing + new_recs, max_fetch

# --- Î©îÏù∏ ---------------------------------------------------
records, last_fetch = load_cache()
print(f"Loaded {len(records)} records; last fetch at {last_fetch}")

latest_time = get_latest_run_time()
if latest_time is None:
    print("No runs at all in project.")
elif last_fetch and latest_time <= last_fetch:
    print("No new runs since last fetch ‚Äì skipping W&B query.")
else:
    # Ïã†Í∑ú Îü∞Ïù¥ ÏûàÏúºÎãà createdAt ÌïÑÌÑ∞Î°ú ÏµúÏÜåÌïúÎßå fetch
    records, new_fetch = fetch_new_records(records, last_fetch or datetime.min.replace(tzinfo=timezone.utc))
    print(f"Fetched {len(records) - len(records):d} new records; new last_fetch={new_fetch}")
    save_cache(records, new_fetch)

# ÌååÏùº ÏµúÏÉÅÎã®Ïóê Ìïú Ï§Ñ Ï∂îÍ∞Ä
TRANSFER_TARGET_ORDER = [
    "ppgbp‚Üíbcg",
    "sensors‚Üíbcg",
    "bcg‚Üíppgbp",
    "sensors‚Üíppgbp",
    "bcg‚Üísensors",
    "ppgbp‚Üísensors",
    "vital_ecg‚Üímimic_ecg",
    "mimic_ecg‚Üívital_ecg"
]

def show_results(shots, sort_by="spdp", chunk_size=3):
    """
    shots: int
    sort_by: "spdp" (ÎÇÆÏùÑÏàòÎ°ù Ï¢ãÏùå) ÎòêÎäî "gal" (ÎÜíÏùÑÏàòÎ°ù Ï¢ãÏùå)
    chunk_size: Ìïú Î≤àÏóê Î≥¥Ïó¨Ï§Ñ transfer_target Ïàò
    """
    assert sort_by in ("spdp", "gal")

    df = pd.DataFrame(records)
    df = df[df.shots == shots]

    # label Î≤°ÌÑ∞Ìôî
    mask = (df.backbone=="bptransformer") & (df.method=="prompt_global")
    df["label"] = df.backbone
    df.loc[mask, "label"] = "bpt+ours"
    df["transfer_target"] = df.transfer + "‚Üí" + df.target

    # 1) best run Ïù∏Îç±Ïä§ Ï∞æÍ∏∞
    df_metric = df[df.metric == sort_by]
    if sort_by == "spdp":
        best_idx = df_metric.groupby(["label","transfer_target"])["value"].idxmin()
    else:
        best_idx = df_metric.groupby(["label","transfer_target"])["value"].idxmax()

    best = df_metric.loc[best_idx, ["label","transfer_target","run_id","value"]].copy()
    best = best.rename(columns={"value": sort_by})

    # 2) Îã§Î•∏ Î©îÌä∏Î¶≠ Í∞í ÎßµÌïë
    other = "gal" if sort_by=="spdp" else "spdp"
    df_other = df[df.metric==other][["run_id","value"]].set_index("run_id")
    best[other] = best["run_id"].map(df_other["value"])

    # 3) pivot ‚Üí Î©ÄÌã∞Ïù∏Îç±Ïä§ Ïª¨Îüº: (metric, transfer_target)
    pivot = best.set_index(["label","transfer_target"])[[sort_by, other]]
    pivot = pivot.unstack("transfer_target")

    # 4) Ïª¨Îüº ÏàúÏÑú Í≥†Ï†ï: transfer_target Ïö∞ÏÑ†, Í∑∏ ÏïàÏóê [sort_by, other] metric
    metrics = [sort_by, other]
    cols = []
    for tt in TRANSFER_TARGET_ORDER:
        for m in metrics:
            cols.append((m, tt))
    pivot = pivot.reindex(columns=pd.MultiIndex.from_tuples(cols), fill_value=float("nan"))

    # 5) Ìñâ ÏàúÏÑú Í≥†Ï†ï
    row_order = ["resnet1d","spectroresnet","mlpbp","bptransformer","bpt+ours"]
    pivot = pivot.reindex(row_order)

    # 6) chunk Îã®ÏúÑÎ°ú ÎÇòÎà†ÏÑú Ï∂úÎ†•
    chunks = [
        TRANSFER_TARGET_ORDER[i : i + chunk_size]
        for i in range(0, len(TRANSFER_TARGET_ORDER), chunk_size)
    ]

    for chunk in chunks:
        # Ìï¥Îãπ chunkÏóê ÏÜçÌïú Ïª¨ÎüºÎßå Í≥®ÎùºÏÑú
        sub = pivot.loc[:, pd.IndexSlice[metrics, chunk]].copy()

        # 7) Í∞Å Ïª¨ÎüºÎ≥Ñ 1Îì± Ïù¥Î™®ÏßÄ Ï∂îÍ∞Ä
        for m in metrics:
            for tt in chunk:
                col = (m, tt)
                vals = sub[col].astype(float)
                best_val = vals.min() if m=="spdp" else vals.max()
                # Î¨∏ÏûêÏó¥ Ìè¨Îß∑ + emoji
                sub[col] = vals.apply(
                    lambda x: f"{x:.2f}" + (" ü•á" if x == best_val else "")
                )

        # 8) Ìëú Ï∂úÎ†•
        header = f"\n=== Shots: {shots} | best by {sort_by} | targets={chunk} ==="
        print(header)
        print(tabulate(sub, headers="keys", tablefmt="grid"))

if __name__=="__main__":
    show_results(5, sort_by="spdp", chunk_size=2)
    # show_results(5, sort_by="gal", chunk_size=2)
